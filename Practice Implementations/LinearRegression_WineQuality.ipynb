{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This noteook will implement gradient descent using multivariate linear regression to predict wine quality. It will provide a full walkthrough of how I implemented the linear regression model, followed by an analysis of the performance of the model with proposed changes to make the model better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from tabulate import tabulate\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data and perform feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data and use feature normalization to ensure gradient descent converges much more quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[0]    X[1]    X[2]    X[3]    X[4]    X[5]    X[6]    X[7]    X[8]    X[9]    X[10]    quality (1-10)\n",
      "------  ------  ------  ------  ------  ------  ------  ------  ------  ------  -------  ----------------\n",
      "   7.4    0.7     0        1.9   0.076      11      34  0.9978    3.51    0.56      9.4                 5\n",
      "   7.8    0.88    0        2.6   0.098      25      67  0.9968    3.2     0.68      9.8                 5\n",
      "   7.8    0.76    0.04     2.3   0.092      15      54  0.997     3.26    0.65      9.8                 5\n",
      "  11.2    0.28    0.56     1.9   0.075      17      60  0.998     3.16    0.58      9.8                 6\n",
      "   7.4    0.7     0        1.9   0.076      11      34  0.9978    3.51    0.56      9.4                 5\n",
      "   7.4    0.66    0        1.8   0.075      13      40  0.9978    3.51    0.56      9.4                 5\n",
      "   7.9    0.6     0.06     1.6   0.069      15      59  0.9964    3.3     0.46      9.4                 5\n",
      "   7.3    0.65    0        1.2   0.065      15      21  0.9946    3.39    0.47     10                   7\n",
      "   7.8    0.58    0.02     2     0.073       9      18  0.9968    3.36    0.57      9.5                 7\n",
      "   7.5    0.5     0.36     6.1   0.071      17     102  0.9978    3.35    0.8      10.5                 5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(open(\"data/wineQuality.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "# Create feature matrix and output variables\n",
    "# Here, X denotes the feature matrix and y is the output\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "m = y.size\n",
    "\n",
    "# print out some data points\n",
    "sample = data[:10, :]\n",
    "table = [column for column in sample]\n",
    "print(tabulate(table, headers=[\"X[0]\", \"X[1]\", \"X[2]\", \"X[3]\", \"X[4]\", \"X[5]\", \"X[6]\", \"X[7]\", \"X[8]\", \"X[9]\", \"X[10]\", \"quality (1-10)\" ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a normalized version of the feature matrix\n",
    "# Parameters: X = feature matrix\n",
    "def featureNormalization(X):\n",
    "    # create mu and sigma vector\n",
    "    # mu[x] denotes the mean value of column x\n",
    "    # sigma[x] denotes the standard deviation of column x\n",
    "    X_normalized = X.copy()\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "    \n",
    "    # set the values of mu and sigma\n",
    "    mu = np.mean(X, axis = 0)\n",
    "    sigma = np.std(X, axis = 0)\n",
    "    X_normalized = (X - mu) / sigma\n",
    "    \n",
    "    # return normalized feature matrix, mu and sigma vector\n",
    "    return X_normalized, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed mean vector:  [ 8.31963727  0.52782051  0.27097561  2.5388055   0.08746654 15.87492183\n",
      " 46.46779237  0.99674668  3.3111132   0.65814884 10.42298311]\n",
      "\n",
      "Computed sigma vector:  [1.74055180e+00 1.79003704e-01 1.94740214e-01 1.40948711e+00\n",
      " 4.70505826e-02 1.04568856e+01 3.28850367e+01 1.88674370e-03\n",
      " 1.54338181e-01 1.69453967e-01 1.06533430e+00]\n"
     ]
    }
   ],
   "source": [
    "# call featureNormalization on the data\n",
    "X_normalized, mu, sigma = featureNormalization(X)\n",
    "\n",
    "print(\"Computed mean vector: \", mu)\n",
    "print(\"\\nComputed sigma vector: \", sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we use the feature matrix to compute the cost function, we must add the intercept term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept term to X\n",
    "X = np.concatenate([np.ones((m, 1)), X_normalized], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must implement the cost function for our multivariate linear regression model. This function computes the average of all the results of our linear hypothesis with inputs from our feature matrix compared to the actual output of our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function for multivariate linear regression\n",
    "# Parameters: X = feature matrix, y = output, theta = parameter vector\n",
    "# Returns: cost = the computed cost of fitting data points using theta, the parameter vector\n",
    "def costFunction(X, y, theta):\n",
    "    \n",
    "    # number of training examples\n",
    "    n = y.shape[0]\n",
    "    cost = 0\n",
    "    # hypothesis\n",
    "    h = X.dot(theta)\n",
    "    \n",
    "    # vectorized equation for cost function\n",
    "    cost = (1/(2 * n)) * np.dot((h - y).T, (h - y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cost function defined above, we now implement the gradient descent algorithm to train the model to find the optimal values for our parameter vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent algorithm\n",
    "# Parameters: X = feature matrix, y = output, theta = feature vector, alpha = learning rate, iterations = number of iterations\n",
    "# Returns: theta: The learned parameter vector\n",
    "#          costVector: a list containign the cost function after each iteration\n",
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    \n",
    "    # copy the theta vector to be updated by gradient descent\n",
    "    theta = theta.copy()\n",
    "    m = y.shape[0]\n",
    "    costVector = []\n",
    " \n",
    "    for i in range(iterations):\n",
    "        theta = theta - (alpha / m) * np.dot((np.dot(X, theta) - y), X)\n",
    "        costVector.append(costFunction(X, y, theta))\n",
    "    \n",
    "    return theta, costVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the algorithm is implemented correctly, we can plot a graph of the learning rate over the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cost')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning rate and number of iterations\n",
    "alpha = 0.005\n",
    "iterations = 1500\n",
    "\n",
    "theta = np.zeros(X[0].shape)\n",
    "theta, costVector = gradientDescent(X, y, theta, alpha, iterations)\n",
    "\n",
    "# plot the gradient descent convergence\n",
    "pyplot.plot(np.arange(len(costVector)), costVector, lw=2)\n",
    "pyplot.xlabel('Number of iterations')\n",
    "pyplot.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With alpha = 0.005 and iterations = 1500, the trained parameters are:\n",
      "\n",
      " [ 5.63296341  0.06208009 -0.18391945 -0.01786227  0.0320261  -0.09068463\n",
      "  0.04331262 -0.10730923 -0.06264955 -0.04634955  0.15953812  0.27393323]\n",
      "\n",
      "The predicted wine value for the following features:\n",
      "\n",
      "fixed acidity = 5.7\n",
      "volatile acidity = 0.29\n",
      "citric acid content = 0.42\n",
      "residual sugar = 2.8\n",
      "chlorides = 0.14\n",
      "free sulfur dioxide = 11.0\n",
      "total sulfur dioxide = 42.0\n",
      "density = 0.9942\n",
      "pH = 3.39\n",
      "sulphates = 0.71\n",
      "alcohol % = 10.3\n",
      "\n",
      "wine quality = 5.757194394118167\n"
     ]
    }
   ],
   "source": [
    "#output the trained parameter vector\n",
    "print('With alpha = {} and iterations = {}, the trained parameters are:\\n\\n {}' .format(alpha, iterations, theta))\n",
    "\n",
    "#Test with random feature values\n",
    "X_test = np.array([5.7, 0.29, 0.42, 2.8, 0.140, 11, 42, 0.9942, 3.39, 0.71, 10.3])\n",
    "print(\"\\nThe predicted wine value for the following features:\\n\\n\"\n",
    "      \"fixed acidity = {}\\n\"\n",
    "      \"volatile acidity = {}\\n\"\n",
    "      \"citric acid content = {}\\n\" \n",
    "      \"residual sugar = {}\\n\"\n",
    "      \"chlorides = {}\\n\" \n",
    "      \"free sulfur dioxide = {}\\n\"\n",
    "      \"total sulfur dioxide = {}\\n\" \n",
    "      \"density = {}\\n\" \n",
    "      \"pH = {}\\n\" \n",
    "      \"sulphates = {}\\n\" \n",
    "      \"alcohol % = {}\\n\"\n",
    "      .format(X_test[0], X_test[1], X_test[2], X_test[3], X_test[4], \n",
    "              X_test[5], X_test[6], X_test[7], X_test[8], X_test[9], X_test[10]))\n",
    "\n",
    "#add intercept term\n",
    "X_test = np.concatenate(([1], X_test))\n",
    "predict = np.dot(X_test1, theta)\n",
    "print(\"wine quality =\", predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
