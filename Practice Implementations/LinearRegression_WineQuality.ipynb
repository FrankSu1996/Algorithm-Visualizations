{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This noteook will implement gradient descent using multivariate linear regression to predict wine quality. It will provide a full walkthrough of how I implemented the linear regression model, followed by an analysis of the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from tabulate import tabulate\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data and perform feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data and use feature normalization to ensure gradient descent converges much more quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[0]    X[1]    X[2]    X[3]    X[4]    X[5]    X[6]    X[7]    X[8]    X[9]    X[10]    quality (1-10)\n",
      "------  ------  ------  ------  ------  ------  ------  ------  ------  ------  -------  ----------------\n",
      "   7.4    0.7     0        1.9   0.076      11      34  0.9978    3.51    0.56      9.4                 5\n",
      "   7.8    0.88    0        2.6   0.098      25      67  0.9968    3.2     0.68      9.8                 5\n",
      "   7.8    0.76    0.04     2.3   0.092      15      54  0.997     3.26    0.65      9.8                 5\n",
      "  11.2    0.28    0.56     1.9   0.075      17      60  0.998     3.16    0.58      9.8                 6\n",
      "   7.4    0.7     0        1.9   0.076      11      34  0.9978    3.51    0.56      9.4                 5\n",
      "   7.4    0.66    0        1.8   0.075      13      40  0.9978    3.51    0.56      9.4                 5\n",
      "   7.9    0.6     0.06     1.6   0.069      15      59  0.9964    3.3     0.46      9.4                 5\n",
      "   7.3    0.65    0        1.2   0.065      15      21  0.9946    3.39    0.47     10                   7\n",
      "   7.8    0.58    0.02     2     0.073       9      18  0.9968    3.36    0.57      9.5                 7\n",
      "   7.5    0.5     0.36     6.1   0.071      17     102  0.9978    3.35    0.8      10.5                 5\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(open(\"data/wineQuality.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "# Create feature matrix and output variables\n",
    "# Here, X denotes the feature matrix and y is the output\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# print out some data points\n",
    "sample = data[:10, :]\n",
    "table = [column for column in sample]\n",
    "print(tabulate(table, headers=[\"X[0]\", \"X[1]\", \"X[2]\", \"X[3]\", \"X[4]\", \"X[5]\", \"X[6]\", \"X[7]\", \"X[8]\", \"X[9]\", \"X[10]\", \"quality (1-10)\" ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a normalized version of the feature matrix\n",
    "def featureNormalization(featureMatrix=X):\n",
    "    # create mu and sigma vector\n",
    "    # mu[x] denotes the mean value of column x\n",
    "    # sigma[x] denotes the standard deviation of column x\n",
    "    X_normalized = X.copy()\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "    \n",
    "    # set the values of mu and sigma\n",
    "    mu = np.mean(X, axis = 0)\n",
    "    sigma = np.std(X, axis = 0)\n",
    "    X_normalized = (X - mu) / sigma\n",
    "    \n",
    "    # return normalized feature matrix, mu and sigma vector\n",
    "    return X_normalized, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed mean vector:  [ 8.31963727  0.52782051  0.27097561  2.5388055   0.08746654 15.87492183\n",
      " 46.46779237  0.99674668  3.3111132   0.65814884 10.42298311]\n",
      "\n",
      "Computed sigma vector:  [1.74055180e+00 1.79003704e-01 1.94740214e-01 1.40948711e+00\n",
      " 4.70505826e-02 1.04568856e+01 3.28850367e+01 1.88674370e-03\n",
      " 1.54338181e-01 1.69453967e-01 1.06533430e+00]\n"
     ]
    }
   ],
   "source": [
    "# call featureNormalization on the data\n",
    "X_normalized, mu, sigma = featureNormalization(X)\n",
    "\n",
    "print(\"Computed mean vector: \", mu)\n",
    "print(\"\\nComputed sigma vector: \", sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we use the feature matrix to compute the cost function, we must add the intercept term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept term to X\n",
    "m = y.size\n",
    "X = np.concatenate([np.ones((m, 1)), X_normalized], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must implement the cost function for our multivariate linear regression model. This function computes the average of all the results of our linear hypothesis with inputs from our feature matrix compared to the actual output of our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function for multivariate linear regression\n",
    "# Parameters: X = feature matrix, y = output, theta = parameter vector\n",
    "# Returns: cost = the computed cost of fitting data points using theta, the parameter vector\n",
    "def costFunction(X, y, theta):\n",
    "    \n",
    "    # number of training examples\n",
    "    n = y.shape[0]\n",
    "    cost = 0\n",
    "    \n",
    "    # hypothesis\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    # vectorized implementation of cost\n",
    "    cost = (1/(2*m)) * np.dot((hypothesis - y).T, (hypothesis - y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cost function defined above, we now implement the gradient descent algorithm to train the model to find the optimal values for our parameter vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.888342020222296, 15.574855653509339, 15.267665735762387, 14.966644639723452, 14.671667376140315, 14.382611536842878, 14.099357239166116, 13.821787071681936, 13.549786041203404, 13.283241521026152, 13.022043200373162, 12.76608303500994, 12.515255198998709, 12.269456037560781, 12.028584021017817, 11.79253969978332, 11.561225660376763, 11.334546482433618, 11.112408696685664, 10.894720743886268, 10.681392934656685, 10.472337410229898, 10.267468104069119, 10.066700704339317, 9.86995261721002, 9.677142930969074, 9.48819238092714, 9.303023315093547, 9.121559660604586, 8.943726890885985, 8.769451993531794, 8.598663438882358, 8.431291149284698, 8.267266469018992, 8.10652213487525, 7.94899224736493, 7.7946122425524615, 7.643318864492245, 7.495050138256741, 7.3497453435422635, 7.207344988838791, 7.067790786150881, 6.931025626257067, 6.79699355449539, 6.665639747062902, 6.536910487817688, 6.410753145571811, 6.287116151864198, 6.165948979202615, 6.047202119764173, 5.930827064544132, 5.816776282942976, 5.705003202782031, 5.595462190738023, 5.4881085331874155, 5.382898417451376, 5.279788913432617, 5.178737955635427, 5.079704325560595, 4.9826476344668755, 4.887528306491165, 4.7943075621194415, 4.702947402000928, 4.613410591098002, 4.5256606431645725, 4.439661805545883, 4.3553790442927705, 4.272778029583602, 4.19182512144733, 4.11248735578117, 4.0347324306566215, 3.9585286929076373, 3.8838451249949726, 3.810651332140799, 3.738917529727815, 3.668614530957321, 3.599713734760643, 3.532187113958686, 3.4660072036642364, 3.4011470899219773, 3.3375803985811814, 3.2752812843961685, 3.2142244203497388, 3.1543849871949234, 3.0957386632104393, 3.0382616141653958, 2.9819304834888496, 2.926722382639954, 2.872614881674477, 2.819586000003616, 2.7676141973411044, 2.7166783648346535, 2.66675781637795, 2.6178322800994, 2.569881890023984, 2.5228871779046242, 2.4768290652195506, 2.431688855332214, 2.387448225810396, 2.3440892209012336, 2.3015942441589035, 2.2599460512218483, 2.219127742736451, 2.1791227574241057, 2.1399148652887954, 2.1014881609622162, 2.0638270571836657, 2.026916278411894, 1.990740854566244, 1.955286114894362, 1.9205376819639692, 1.8864814657760505, 1.8531036579970712, 1.820390726307702, 1.7883294088657031, 1.7569067088806574, 1.726109889298185, 1.6959264675915084, 1.6663442106580753, 1.6373511298191603, 1.6089354759203094, 1.5810857345305929, 1.5537906212386288, 1.5270390770434206, 1.5008202638380785, 1.4751235599845065, 1.4499385559772622, 1.4252550501946948, 1.401063044735658, 1.3773527413400084, 1.3541145373912316, 1.3313390219994718, 1.3090169721633844, 1.2871393490091922, 1.2656972941053544, 1.24468212585136, 1.224085335939106, 1.2038985858854074, 1.1841137036341916, 1.1647226802269586, 1.1457176665401463, 1.1270909700880167, 1.1088350518897638, 1.0909425233995262, 1.0734061434980373, 1.0562188155446708, 1.039373584488655, 1.022863634038259, 1.006682283886794, 0.9908229869942473, 0.9752793269234785, 0.9600450152298209, 0.9451138889030528, 0.9304799078606486, 0.9161371524913121, 0.9020798212477337, 0.888302228287625, 0.8747988011620113, 0.8615640785498773, 0.848592708038182, 0.8358794439463756, 0.8234191451944877, 0.8112067732139229, 0.7992373899001106, 0.7875061556061502, 0.7760083271766346, 0.7647392560208542, 0.7536943862245662, 0.7428692526995708, 0.7322594793703318, 0.7218607773968901, 0.7116689434333443, 0.7016798579211859, 0.6918894834167776, 0.6822938629523112, 0.6728891184295434, 0.6636714490456719, 0.6546371297507013, 0.6457825097356623, 0.6371040109510647, 0.6285981266549883, 0.6202614199901973, 0.6120905225897136, 0.6040821332102688, 0.5962330163930752, 0.5885400011513751, 0.5809999796842192, 0.5736099061159601, 0.5663667952609351, 0.5592677214128433, 0.5523098171583115, 0.5454902722141715, 0.5388063322879661, 0.5322552979612339, 0.5258345235950915, 0.5195414162576882, 0.5133734346730876, 0.5073280881911392, 0.501402935777936, 0.49559558502642664, 0.48990369118679394, 0.4843249562161947, 0.4788571278474801, 0.47349799867650744, 0.46824540526767966, 0.4630972272773463, 0.45805138659470096, 0.45310584649983526, 0.44825861083859936, 0.4435077232139359, 0.4388512661933554, 0.4342873605322289, 0.4298141644125904, 0.42542987269712396, 0.4211327161980476, 0.41692096096058, 0.4127929075607152, 0.40874689041700374, 0.4047812771160722, 0.4008944677515981, 0.3970848942764723, 0.3933510198678953, 0.38969133830513175, 0.38610437335968295, 0.3825886781976278, 0.3791428347938809, 0.37576545335814077, 0.3724551717722869, 0.3692106550389968, 0.3660305947413648, 0.3629137085132973, 0.35985873952046943, 0.35686445595163746, 0.35392965052009157, 0.3510531399750561, 0.3482337646228267, 0.3454703878574649, 0.34276189570084475, 0.34010719635187353, 0.3375052197447021, 0.3349549171157396, 0.3324552605793067, 0.3300052427117469, 0.3276038761438276, 0.3252501931612706, 0.32294324531324814, 0.3206821030286739, 0.31846585524015664, 0.3162936090154408, 0.31416448919619805, 0.3120776380440168, 0.31003221489345, 0.3080273958119782, 0.30606237326674796, 0.30413635579795695, 0.30224856769874514, 0.3003982487014667, 0.29858465367021547, 0.29680705229947907, 0.2950647288187943, 0.2933569817032943, 0.2916831233900192, 0.29004247999988336, 0.2884343910651796, 0.28685820926251715, 0.2853133001510802, 0.28379904191609817, 0.2823148251174336, 0.2808600524431758, 0.2794341384681433, 0.2780365094172017, 0.2766666029332935, 0.275323867850092, 0.27400776396918386, 0.2727177618416902, 0.27145334255424025, 0.27021399751920616, 0.26899922826912065, 0.26780854625518413, 0.2666414726497934, 0.26549753815299815, 0.2643762828028152, 0.26327725578932165, 0.2622000152724502, 0.2611441282034152, 0.2601091701496947, 0.2590947251234988, 0.25810038541365526, 0.25712575142084315, 0.2561704314961086, 0.2552340417825966, 0.2543162060604351, 0.2534165555947099, 0.2525347289864642, 0.25167037202667003, 0.25082313755310515, 0.2499926853100791, 0.24917868181095812, 0.24838080020342224, 0.24759872013740883, 0.24683212763568613, 0.24608071496700257, 0.2453441805217635, 0.2446222286901841, 0.24391456974286777, 0.2432209197137617, 0.2425410002854469, 0.24187453867670783, 0.24122126753234432, 0.24058092481517562, 0.23995325370019632, 0.23933800247083836, 0.23873492441729974, 0.23814377773689818, 0.23756432543640707, 0.23699633523633773, 0.23643957947712677, 0.23589383502719177, 0.23535888319281578, 0.23483450962983018, 0.23432050425705087, 0.23381666117144234, 0.23332277856496772, 0.23283865864309605, 0.2323641075449316, 0.23189893526493222, 0.23144295557618846, 0.23099598595522863, 0.23055784750832106, 0.23012836489924213, 0.22970736627848662, 0.22929468321388222, 0.2288901506225888, 0.22849360670444974, 0.22810489287667138, 0.22772385370980103, 0.22735033686498096, 0.22698419303245165, 0.2266252758712774, 0.22627344195027405, 0.2259285506901113, 0.22559046430656712, 0.22525904775491346, 0.22493416867540766, 0.2246156973398691, 0.2243035065993197, 0.2239974718326658, 0.2236974708964024, 0.2234033840753191, 0.22311509403418486, 0.22283248577039716, 0.2225554465675721, 0.22228386595005853, 0.22201763563835794, 0.22175664950543122, 0.2215008035338754, 0.22124999577395243, 0.22100412630245395, 0.2207630971823837, 0.22052681242344382, 0.22029517794330603, 0.2200681015296554, 0.21984549280298749, 0.21962726318014802, 0.21941332583859732, 0.2192035956813839, 0.21899798930281986, 0.21879642495483556, 0.2185988225140042, 0.21840510344922462, 0.21821519079004612, 0.21802900909562295, 0.21784648442428828, 0.21766754430373192, 0.21749211770177374, 0.21732013499771616, 0.21715152795426806, 0.2169862296900288, 0.2168241746525166, 0.21666529859173717, 0.21650953853427474, 0.21635683275790252, 0.21620712076669538, 0.2160603432666391, 0.2159164421417246, 0.21577536043051837, 0.21563704230319905, 0.21550143303904978, 0.21536847900440087, 0.21523812763100836, 0.2151103273948654, 0.2149850277954344, 0.21486217933529086, 0.21474173350017592, 0.21462364273944146, 0.21450786044688755, 0.21439434094198034, 0.2142830394514423, 0.2141739120912111, 0.21406691584875545, 0.21396200856574416, 0.21385914892105898, 0.21375829641414654, 0.2136594113487008, 0.21356245481667027, 0.21346738868258328, 0.21337417556818558, 0.21328277883738364, 0.21319316258148707, 0.2131052916047445, 0.21301913141016818, 0.21293464818563979, 0.2128518087902928, 0.2127705807411659, 0.2126909322001222, 0.21261283196102887, 0.21253624943718974, 0.21246115464903195, 0.2123875182120333, 0.21231531132489256, 0.21224450575793494, 0.2121750738417458, 0.21210698845603407, 0.2120402230187147, 0.21197475147520858, 0.21191054828795672, 0.21184758842614124, 0.21178584735561237, 0.21172530102901485, 0.21166592587611122, 0.21160769879429706, 0.21155059713930593, 0.2114945987160985, 0.21143968176993236, 0.21138582497761116, 0.21133300743890576, 0.21128120866814837, 0.21123040858599135, 0.21118058751133192, 0.21113172615339665, 0.21108380560398363, 0.2110368073298588, 0.21099071316530377, 0.21094550530481113, 0.2109011662959266, 0.210857679032232, 0.2108150267464682, 0.21077319300379582, 0.21073216169518835, 0.2106919170309583, 0.21065244353441068, 0.21061372603562423, 0.21057574966535467, 0.21053849984906037, 0.21050196230104712, 0.21046612301872758, 0.2104309682769971, 0.2103964846227199, 0.21036265886932584, 0.21032947809151376, 0.2102969296200604, 0.21026500103673376, 0.21023368016930594, 0.21020295508666542, 0.2101728140940285, 0.2101432457282431, 0.21011423875318758, 0.21008578215526122, 0.2100578651389639, 0.2100304771225633, 0.21000360773384957, 0.2099772468059724, 0.20995138437336291, 0.20992601066773606, 0.20990111611417095, 0.20987669132727205, 0.2098527271074039, 0.2098292144370021, 0.20980614447695808, 0.20978350856307457, 0.20976129820259168, 0.20973950507078384, 0.20971812100762136, 0.20969713801450182, 0.209676548251043, 0.20965634403194244, 0.2096365178238982, 0.20961706224259014, 0.20959797004972247, 0.20957923415012514, 0.2095608475889112, 0.2095428035486931, 0.20952509534685176, 0.20950771643286262, 0.20949066038567227, 0.2094739209111292, 0.2094574918394645, 0.20944136712282307, 0.2094255408328442, 0.20941000715828917, 0.20939476040271715, 0.2093797949822072, 0.20936510542312498, 0.20935068635993395, 0.20933653253305048, 0.20932263878674093, 0.20930900006706177, 0.20929561141983938, 0.20928246798869038, 0.20926956501308178, 0.2092568978264287, 0.2092444618542311, 0.20923225261224654, 0.20922026570470065, 0.20920849682253045, 0.20919694174166603, 0.2091855963213438, 0.20917445650245395, 0.2091635183059221, 0.2091527778311209, 0.209142231254314, 0.20913187482713272, 0.20912170487508014, 0.20911171779606644, 0.2091019100589736, 0.2090922782022483, 0.2090828188325219, 0.2090735286232594, 0.20906440431343365, 0.20905544270622797, 0.20904664066776216, 0.20903799512584498, 0.2090295030687525, 0.20902116154402836, 0.20901296765730965, 0.20900491857117573, 0.20899701150401986, 0.20898924372894268, 0.2089816125726686, 0.20897411541448324, 0.20896674968519177, 0.20895951286609862, 0.20895240248800667, 0.20894541613023657, 0.2089385514196666, 0.20893180602978936, 0.20892517767978977, 0.20891866413363863, 0.2089122631992076, 0.20890597272739803, 0.20889979061128985, 0.20889371478530636, 0.2088877432243954, 0.20888187394322727, 0.20887610499540785, 0.20887043447270887, 0.20886486050431138, 0.208859381256066, 0.20885399492976683, 0.20884869976244022, 0.2088434940256477, 0.20883837602480218, 0.20883334409849885, 0.20882839661785815, 0.20882353198588216, 0.2088187486368251, 0.20881404503557305, 0.20880941967704064, 0.20880487108557474, 0.20880039781437415, 0.20879599844491842, 0.20879167158640843, 0.20878741587521865, 0.2087832299743604, 0.20877911257295342, 0.20877506238571272, 0.20877107815244, 0.20876715863752934, 0.208763302629481, 0.20875950894042472, 0.20875577640565313, 0.2087521038831645, 0.20874849025321301, 0.20874493441787062, 0.20874143530059522, 0.20873799184580857, 0.20873460301848237, 0.2087312678037322, 0.20872798520642055, 0.2087247542507657, 0.20872157397996155, 0.20871844345580087, 0.20871536175831018, 0.20871232798538875, 0.2087093412524564, 0.20870640069210822, 0.2087035054537745, 0.20870065470339033, 0.20869784762306876, 0.20869508341078213, 0.20869236128005011, 0.20868968045963202, 0.20868704019322698, 0.2086844397391795, 0.2086818783701908, 0.20867935537303567, 0.2086768700482855, 0.20867442171003608, 0.20867200968564126, 0.20866963331545235, 0.2086672919525612, 0.2086649849625506, 0.20866271172324685, 0.20866047162447976, 0.20865826406784616, 0.2086560884664784, 0.2086539442448172, 0.20865183083838854, 0.20864974769358696, 0.20864769426746063, 0.2086456700275024, 0.20864367445144444, 0.2086417070270563, 0.20863976725194805, 0.20863785463337728, 0.2086359686880589, 0.20863410894197879, 0.2086322749302137, 0.20863046619675013, 0.20862868229431103, 0.2086269227841841, 0.20862518723605275, 0.20862347522783206, 0.20862178634550718, 0.20862012018297488, 0.20861847634188785, 0.20861685443150363, 0.20861525406853415, 0.20861367487700075, 0.20861211648809005, 0.20861057854001405, 0.2086090606778719, 0.20860756255351567, 0.20860608382541726, 0.20860462415853973, 0.2086031832242085, 0.2086017606999894, 0.20860035626956366, 0.20859896962261032, 0.2085976004546879, 0.20859624846711972, 0.20859491336688113, 0.2085935948664885, 0.20859229268389187, 0.20859100654236767, 0.208589736170416, 0.20858848130165658, 0.20858724167473097, 0.20858601703320273, 0.20858480712546287, 0.20858361170463402, 0.20858243052847927, 0.20858126335931157, 0.2085801099639042, 0.20857897011340476, 0.20857784358324907, 0.20857673015307843, 0.20857562960665688, 0.20857454173179157, 0.20857346632025361, 0.20857240316770115, 0.20857135207360308, 0.20857031284116642, 0.20856928527726173, 0.20856826919235288, 0.20856726440042775, 0.20856627071892822, 0.2085652879686844, 0.20856431597384853, 0.20856335456182945, 0.20856240356323116, 0.20856146281178986, 0.20856053214431222, 0.20855961140061785, 0.20855870042347968, 0.20855779905856686, 0.20855690715438854, 0.20855602456223915, 0.20855515113614495, 0.20855428673280987, 0.20855343121156464, 0.20855258443431618, 0.20855174626549686, 0.20855091657201677, 0.20855009522321508, 0.2085492820908134, 0.20854847704886975, 0.20854767997373383, 0.20854689074400215, 0.20854610924047498, 0.20854533534611472, 0.20854456894600243, 0.20854380992729887, 0.20854305817920352, 0.20854231359291522, 0.20854157606159437, 0.20854084548032498, 0.2085401217460766, 0.20853940475767077, 0.20853869441574238, 0.20853799062270664, 0.2085372932827251, 0.20853660230167084, 0.20853591758709722, 0.20853523904820437, 0.2085345665958083, 0.2085339001423105, 0.2085332396016661, 0.20853258488935575, 0.20853193592235547, 0.20853129261910883, 0.20853065489949799, 0.2085300226848173, 0.20852939589774575, 0.20852877446232002, 0.20852815830391058, 0.20852754734919368, 0.20852694152612897, 0.2085263407639333, 0.20852574499305748, 0.20852515414516326, 0.20852456815309958, 0.20852398695088048, 0.20852341047366282, 0.20852283865772503, 0.20852227144044544, 0.20852170876028142, 0.20852115055674922, 0.20852059677040413, 0.20852004734282023, 0.20851950221657145, 0.20851896133521342, 0.20851842464326373, 0.2085178920861844, 0.2085173636103642, 0.20851683916310146, 0.2085163186925861, 0.2085158021478835, 0.20851528947891818, 0.20851478063645718, 0.2085142755720948, 0.20851377423823672, 0.2085132765880847, 0.2085127825756224, 0.20851229215559972, 0.20851180528351948, 0.20851132191562277, 0.20851084200887549, 0.20851036552095423, 0.20850989241023438, 0.2085094226357753, 0.20850895615730902, 0.20850849293522755, 0.2085080329305699, 0.20850757610501078, 0.20850712242084868, 0.20850667184099425, 0.20850622432895857, 0.20850577984884314, 0.20850533836532786, 0.20850489984366063, 0.20850446424964778, 0.20850403154964256, 0.20850360171053553, 0.20850317469974497, 0.20850275048520708, 0.20850232903536606, 0.20850191031916562, 0.2085014943060386, 0.20850108096589978, 0.20850067026913535, 0.20850026218659487, 0.208499856689584, 0.20849945374985424, 0.208499053339596, 0.20849865543143087, 0.20849825999840269, 0.208497867013971, 0.20849747645200256, 0.2084970882867654, 0.2084967024929195, 0.20849631904551197, 0.20849593791996784, 0.20849555909208553, 0.2084951825380284, 0.2084948082343187, 0.20849443615783156, 0.2084940662857883, 0.20849369859574973, 0.20849333306561174, 0.20849296967359734, 0.20849260839825173, 0.2084922492184372, 0.20849189211332614, 0.20849153706239665, 0.2084911840454265, 0.2084908330424883, 0.20849048403394368, 0.20849013700043906, 0.20848979192289935, 0.20848944878252454, 0.2084891075607837, 0.2084887682394108, 0.20848843080040008, 0.2084880952260011, 0.2084877614987148, 0.2084874296012888, 0.2084870995167134, 0.20848677122821682, 0.20848644471926103, 0.2084861199735391, 0.20848579697496925, 0.2084854757076923, 0.20848515615606697, 0.20848483830466719, 0.2084845221382772, 0.20848420764188866, 0.20848389480069734, 0.2084835836000983, 0.20848327402568442, 0.20848296606324127, 0.20848265969874524, 0.2084823549183591, 0.2084820517084296, 0.20848175005548394, 0.2084814499462274, 0.20848115136753903, 0.20848085430647043, 0.20848055875024157, 0.20848026468623834, 0.2084799721020099, 0.2084796809852657, 0.2084793913238732, 0.20847910310585513, 0.2084788163193857, 0.2084785309527909, 0.20847824699454282, 0.20847796443325972, 0.2084776832577017, 0.2084774034567699, 0.20847712501950333, 0.20847684793507668, 0.20847657219279866, 0.20847629778210894, 0.20847602469257673, 0.20847575291389847, 0.20847548243589578, 0.20847521324851326, 0.20847494534181663, 0.20847467870599093, 0.20847441333133857, 0.20847414920827687, 0.20847388632733754, 0.20847362467916317, 0.20847336425450702, 0.2084731050442301, 0.20847284703930016, 0.20847259023078996, 0.2084723346098751, 0.20847208016783272, 0.20847182689604063, 0.20847157478597414, 0.20847132382920616, 0.20847107401740442, 0.20847082534233127, 0.20847057779584083, 0.20847033136987858, 0.20847008605647943, 0.20846984184776662, 0.20846959873595058, 0.20846935671332723, 0.20846911577227628, 0.20846887590526086, 0.20846863710482588, 0.20846839936359676, 0.2084681626742779, 0.20846792702965197, 0.20846769242257898, 0.20846745884599402, 0.20846722629290745, 0.20846699475640304, 0.20846676422963664, 0.20846653470583623, 0.20846630617829925, 0.2084660786403934, 0.20846585208555413, 0.20846562650728387, 0.2084654018991524, 0.20846517825479413, 0.20846495556790795, 0.20846473383225644, 0.208464513041665, 0.20846429319002, 0.2084640742712693, 0.20846385627942052, 0.20846363920854027, 0.20846342305275342, 0.2084632078062427, 0.2084629934632469, 0.20846278001806115, 0.2084625674650352, 0.20846235579857333, 0.20846214501313395, 0.20846193510322705, 0.2084617260634159, 0.20846151788831427, 0.2084613105725873, 0.20846110411094934, 0.20846089849816485, 0.20846069372904652, 0.20846048979845505, 0.2084602867012987, 0.20846008443253183, 0.20845988298715568, 0.2084596823602164, 0.20845948254680516, 0.20845928354205773, 0.2084590853411528, 0.20845888793931291, 0.20845869133180295, 0.20845849551392978, 0.20845830048104155, 0.20845810622852784, 0.20845791275181766, 0.208457720046381, 0.2084575281077265, 0.20845733693140214, 0.20845714651299402, 0.20845695684812574, 0.2084567679324592, 0.20845657976169243, 0.20845639233156074, 0.20845620563783449, 0.2084560196763205, 0.2084558344428601, 0.2084556499333297, 0.20845546614363977, 0.2084552830697353, 0.20845510070759365, 0.2084549190532261, 0.2084547381026755, 0.208454557852019, 0.20845437829736316, 0.20845419943484783, 0.20845402126064333, 0.20845384377095041, 0.20845366696200082, 0.20845349083005585, 0.2084533153714066, 0.2084531405823737, 0.20845296645930672, 0.20845279299858355, 0.20845262019661087, 0.20845244804982288, 0.20845227655468185, 0.20845210570767708, 0.20845193550532548, 0.2084517659441701, 0.20845159702078037, 0.2084514287317524, 0.20845126107370782, 0.2084510940432943, 0.2084509276371839, 0.20845076185207462, 0.20845059668468818, 0.208450432131772, 0.20845026819009657, 0.20845010485645704, 0.20844994212767176, 0.20844978000058284, 0.20844961847205543, 0.2084494575389775, 0.2084492971982599, 0.20844913744683563, 0.20844897828166017, 0.20844881969971082, 0.2084486616979866, 0.20844850427350797, 0.2084483474233169, 0.20844819114447644, 0.20844803543407012, 0.20844788028920216, 0.20844772570699768, 0.2084475716846018, 0.20844741821917914, 0.20844726530791483, 0.20844711294801302, 0.20844696113669767, 0.2084468098712119, 0.20844665914881777]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent algorithm\n",
    "# Parameters: X\n",
    "# Returns: theta: The learned parameter vector\n",
    "#          costVector: a list containign the cost function after each iteration\n",
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    \n",
    "    # copy the theta vector to be updated by gradient descent\n",
    "    theta = theta.copy()\n",
    "    costVector = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)\n",
    "        costVector.append(costFunction(X, y, theta))\n",
    "    \n",
    "    return theta, costVector\n",
    "\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "theta = np.zeros(X[1].shape)\n",
    "theta, costVector = gradientDescent(X, y, theta, alpha, iterations)\n",
    "print(costVector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
